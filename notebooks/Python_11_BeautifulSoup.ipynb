{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnSenina/Python_CL_2023/blob/main/notebooks/Python_11_BeautifulSoup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Скрейперы, краулеры, парсеры\n",
        "\n",
        "Веб-скрейпинг = краулинг + парсинг (приблизительно!)\n",
        "\n",
        "Сегодня веб-скрейпинг и парсинг используются как синонимы"
      ],
      "metadata": {
        "id": "tQiBPzaI6F4y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2LDC-PbXhtR"
      },
      "source": [
        "В этой тетрадке мы поговорим о способах собрать свой датасет для исследований: откуда брать данные, как их собирать и как хранить."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHJEqIWOqGoj"
      },
      "source": [
        "Существует несколько библиотек(модулей) для работы с веб-страничками, сегодня мы будем использовать requests для доступа к веб-страничкам и Beautiful Soup для работы с содержимым html-документов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T07:43:42.215240Z",
          "start_time": "2020-12-19T07:43:42.162193Z"
        },
        "id": "ncJ-uAqjDNkC",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install requests #ставим модуль requests\n",
        "\n",
        "# ставим модуль beautifulsoup, самая последняя версия - четвертая\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "# позже понадобится\n",
        "!pip install fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:18:38.198003Z",
          "start_time": "2020-12-19T08:18:38.186002Z"
        },
        "id": "XT5G41FYnr7a"
      },
      "outputs": [],
      "source": [
        "# импортируем модули в тетрадку\n",
        "\n",
        "import requests as rq # если честно, такое сокращение есть, но не повсеместно используется\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from fake_useragent import UserAgent\n",
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIvQNBqUYwE8"
      },
      "source": [
        "# Как работать с веб-страничками"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28Oy0NnLnk0L"
      },
      "source": [
        "### Шаг 1.\n",
        "\n",
        "Создадим переменную ```url``` и сохраним в нее адрес какой-нибудь html-страницы: например, учебной страницы, созданной в Вышке\n",
        "\n",
        "обратите внимание, что адрес прописываем в кавычках, как строку"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:22:23.420826Z",
          "start_time": "2020-12-19T08:22:23.416822Z"
        },
        "id": "WzqN6Ss_aujY"
      },
      "outputs": [],
      "source": [
        "url = 'https://online.hse.ru/python-as-foreign/1/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysgl6VmpKjxu"
      },
      "source": [
        "В модуле requests есть метод request.get(), который сохраняет ответ сервера на наш реквест. Мы применим его к переменной url, куда сохранен путь к странице.\n",
        "Сохраним результат в переменную page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:46:54.806718Z",
          "start_time": "2020-12-19T08:46:54.506720Z"
        },
        "id": "O8AH4kDxqKvF"
      },
      "outputs": [],
      "source": [
        "page = rq.get(url)\n",
        "\n",
        "print(page) # посмотрим на код ответа, если 200, все хорошо\n",
        "print(type(page))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Y2C39Gp0yK"
      },
      "source": [
        "код 200 сообщает, что страница загружена успешно\n",
        "*(коды, начинающиеся с 2, обычно указывают на успешное выполнение операции, а коды, начинающиеся с 4 или 5, сообщают об ошибке)*\n",
        "\n",
        "Узнать больше о кодах состояния HTTP  можно [по этой ссылке.](https://www.w3.org/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-01#Status-Codes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# есть еще одна команда, чтобы получить код в виде числа:\n",
        "print(page.status_code)\n",
        "print(type(page.status_code))"
      ],
      "metadata": {
        "id": "OgWJ0rs36tsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gg53fJKNObR"
      },
      "source": [
        "Следующим шагом нужно получить доступ к текстовому содержимому веб-файлов.\n",
        "\n",
        "Здесь нам поможет page.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:47:01.231718Z",
          "start_time": "2020-12-19T08:47:01.108719Z"
        },
        "id": "md61SNPYu_jL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(page.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page.encoding = 'utf-8' # если видите нечитаемые символы, страничку можно перекодировать\n",
        "print(page.text)"
      ],
      "metadata": {
        "id": "zXIpm0FkVY3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5cEfTSEYhcY"
      },
      "source": [
        "### Шаг2\n",
        "\n",
        "Поработаем с текстом на страничке"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcVMhwpHNxE5"
      },
      "source": [
        "Мы получили текст страницы (со всеми html-тегами), однако его неудобно прочитать в таком виде.\n",
        "\n",
        "Здесь нам понадобится Beautiful Soup, модуль для html-парсинга: он сделает текст веб-страницы, извлеченный с помощью Requests, более читаемым, потому что создает дерево синтаксического разбора из проанализированных тэгов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T09:20:00.809844Z",
          "start_time": "2020-12-19T09:20:00.351849Z"
        },
        "id": "GOBPlBro3AQR"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(page.text, features=\"html.parser\") # сохраним результат в переменную soup\n",
        "# soup = BeautifulSoup(page.text, \"lxml\") - быстрый сбор информации, но возможны ошибки в программе при проблеме с тегами"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup)"
      ],
      "metadata": {
        "id": "1k5HxqVEhxxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:47:09.815788Z",
          "start_time": "2020-12-19T08:47:09.553787Z"
        },
        "id": "HigX0lNr3P1Z",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(soup.prettify()) # показывает нашу страницу в красивом виде"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjED1MGgqGot"
      },
      "source": [
        "### Шаг3\n",
        "время доставать тэги - но сначала...\n",
        "\n",
        "### Немного про html\n",
        "\n",
        "Перейдем на сайт [w3schools.com](https://www.w3schools.com/Html/) и откроем раздел Try it yourself.\n",
        "\n",
        "Задание 1:\n",
        "\n",
        "1.   внутри тэгов h1 напишите ваши Ф.И.О.\n",
        "\n",
        "2.   напишите подзаголовок \"О себе\", добавив тэги h2\n",
        "\n",
        "3.   внутри тэгов p напишите \"Я учусь создавать html-страницы.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2\n",
        "\n",
        "Добавим таблицу\n",
        "\n",
        "<table border=\"1\">\n",
        "<tr>\n",
        "<th>Фамилия</th>\n",
        "<th>Имя</th>\n",
        "<th>Возраст</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<td></td>\n",
        "<td></td>\n",
        "<td></td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "1. Заполните строку таблицы\n",
        "\n",
        "2. Добавьте еще один столбец и заполните его"
      ],
      "metadata": {
        "id": "MqiVxN6jEphg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Бонус! pandas и сам умеет собирать таблицы с сайтов, без rq, BeautifulSoup"
      ],
      "metadata": {
        "id": "x3x43B4Qypk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "banks = pd.read_html('https://cbr.ru/currency_base/daily/')\n",
        "print(banks[0]) # возвращается список датафреймов - потому что вдруг таблиц на сайте несколько"
      ],
      "metadata": {
        "id": "ANg-jm6OxiOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(banks[0]) # 0 объект списка - наша единственная табличка на странице\n",
        "data.to_csv(\"banks.csv\")"
      ],
      "metadata": {
        "id": "xgLK7BdkxxZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "KGL_4TeXeELO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVR8OVuRUaO1"
      },
      "source": [
        "###Вернемся к тегам:\n",
        "\n",
        "предыдущие шаги со страницей позволили привести веб-страничку к виду, где содержание каждого тега написано с новой строки.\n",
        "\n",
        "Некоторые теги полезны для конкретной задачи (там текст), некоторые - не очень (например, мета-данные, картинки и тд)\n",
        "\n",
        "Извлечь одинаковые теги со страницы можно с помощью метода find_all(). Он похож на метод регулярок, с которым мы работали: он вернет все экземпляры данного тега в документе. Нужно прописать в скобках метода нужный тег.\n",
        "\n",
        "### Самые популярные теги:\n",
        "    <h1-h6> - заголовки\n",
        "    <div> для целых \"блоков\" странички\n",
        "    <li> список с перечислением\n",
        "    <p> для текста\n",
        "    <a> для гиперссылок\n",
        "    <img> для изображений"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.find('a')) # найди тег <a></a>"
      ],
      "metadata": {
        "id": "8MxNQVzbyMXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.find('a').text) # найди текст внутри тега <a>some text...</a>"
      ],
      "metadata": {
        "id": "4xpGlhU0yO7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.find('a').get('href')) # найди текст атрибута тега <a href=\"ссылка\"></a>\n",
        "# аналогично:\n",
        "print(soup.find('a')['href']) # суп превращает атрибуты в словарь"
      ],
      "metadata": {
        "id": "1ki1k6qUyT2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.find('a').parent.name) # покажи, как зовут родительский тег\n",
        "# тег верхнего уровня в древе\n",
        "\n",
        "# <body>\n",
        "# <a></a>\n",
        "# <body>"
      ],
      "metadata": {
        "id": "OPeKKqsdyiNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9jO4I8JsqGou"
      },
      "outputs": [],
      "source": [
        "soup.find_all(\"a\") # возвращает все найденные теги в виде списка\n",
        "# print(soup.find_all(\"a\")) # для PyCharm\n",
        "\n",
        "# попробуйте теги body, title и др."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9ZsmEvLqGov"
      },
      "source": [
        "А так можно достать нужные части тега (напимер, текст)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find_all('a')[0].text\n",
        "# print(soup.find_all('a')[0].text)"
      ],
      "metadata": {
        "id": "9lGQ7aH1o0h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-19T08:50:08.027471Z",
          "start_time": "2020-12-19T08:50:08.007477Z"
        },
        "id": "N0RXeVL33W7I",
        "tags": []
      },
      "outputs": [],
      "source": [
        "for x in soup.find_all('a'):\n",
        "    print(x.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in soup.find_all('a'):\n",
        "  if i.parent.name == 'body': # так можно использовать при парсинге\n",
        "    print(i.text, i.get('href'), sep=', ')"
      ],
      "metadata": {
        "id": "kAcg3gfrzDOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JfaHd0lqGov"
      },
      "outputs": [],
      "source": [
        "# Весь текст на страничке за раз можно достать еще и так\n",
        "print(soup.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPczVAd5qGov"
      },
      "source": [
        "## Как создать корпус"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euAwh4wPqGow"
      },
      "source": [
        "Итак, мы определили нужные теги и напарсили необходимые данные. Пора сохранить их в файл."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuqOyfJwqGow"
      },
      "outputs": [],
      "source": [
        "# в .txt\n",
        "\n",
        "with open(\"HP.txt\", \"w\") as file:\n",
        "    for x in soup.find_all(\"a\"):\n",
        "        t = x.text.strip() + \"\\n\"\n",
        "        file.write(t)\n",
        "\n",
        "# необязательно, но можем скорретировать информацию, которую записываем"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdUmUfmaqGow"
      },
      "outputs": [],
      "source": [
        "#   или в .csv\n",
        "\n",
        "names = []\n",
        "for x in soup.find_all(\"a\"): # распакованный текст из тегов\n",
        "  names.append(x.text) # теги отбрось\n",
        "\n",
        "links = []\n",
        "for link in soup.find_all(\"a\"):\n",
        "  links.append(link.get('href')) # вот так можно достать ссылку из тега a\n",
        "\n",
        "print(names)\n",
        "print(links)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "for i in range(len(names)): # списки равной длины, переберем поиндексно\n",
        "  l = []\n",
        "  l.append(names[i])\n",
        "  if links[i].startswith('https'):\n",
        "    l.append(links[i]) # внешняя ссылка с https\n",
        "  else:\n",
        "    l.append('https://online.hse.ru/python-as-foreign/1/' + links[i])\n",
        "  data.append(l)\n",
        "\n",
        "# print(data)\n",
        "data"
      ],
      "metadata": {
        "id": "_MsSRmwgY4Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"name\", \"url\"])\n",
        "\n",
        "df\n",
        "# print(df) #для PyCharm"
      ],
      "metadata": {
        "id": "G4ZdcBlUZU06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqYjNU9dqGox"
      },
      "outputs": [],
      "source": [
        "# сохраняем\n",
        "df.to_csv(\"HP.csv\", encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обычно при парсинге приходится делать так:\n",
        "\n",
        "* соберем все гиперссылки на отдельные страницы\n",
        "* с каждой страницы соберем текст"
      ],
      "metadata": {
        "id": "8yRoH07Iv6Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# перейдём по всем ссылкам и соберём тексты\n",
        "texts = [] # сохранять будем сюда\n",
        "for i in links: # напомню, что ссылка у нас такая \"1.html\"\n",
        "  page = rq.get('https://online.hse.ru/python-as-foreign/1/' + i) # добаляем к ссылке доменное имя / ссылку раздела на сайте\n",
        "  page.encoding = 'utf-8' # перекодируем\n",
        "\n",
        "  # передаем в суп, чтобы работать с тегами\n",
        "  soup = BeautifulSoup(page.text, features=\"html.parser\")\n",
        "\n",
        "  texts.append(soup.text) # текст из всей странички\n",
        "print(texts)"
      ],
      "metadata": {
        "id": "YVSCw2weWsnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text\"] = texts # добавь в датафрейм список текстов\n",
        "df # print(df)"
      ],
      "metadata": {
        "id": "iy_kD44sYlGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdOaCoseqGoy"
      },
      "source": [
        "## Практика\n",
        "\n",
        "давайте попарсим другой адрес и вытащим оттуда весь текст"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRw_mCmXqGoy"
      },
      "outputs": [],
      "source": [
        "url = \"https://en.wikipedia.org/wiki/Welsh_Corgi\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код ниже\n",
        "# передаем в rq\n"
      ],
      "metadata": {
        "id": "csD84n82cBt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# передаем в BeautifulSoup и извлекаем весь текст\n"
      ],
      "metadata": {
        "id": "QDv9Tgz-cF2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# попробуйте спарсить только обычный текст параграфа без лишних переносов строки и лишнего текста со страницы\n",
        "for i in soup.find_all('p'):\n",
        "  print(i.text.strip())"
      ],
      "metadata": {
        "id": "yacwbHHea7KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Бонус: пример парсинга реального сайта\n",
        "\n",
        "Как заставить код спарсить несколько страничек (мы ранее реализовали это в парсинге учебной страничке по ГП)"
      ],
      "metadata": {
        "id": "kwtSDxP_cLOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://nplus1.ru/' # сохраняем\n",
        "page = rq.get(url) # загружаем страницу по ссылке\n",
        "print(page.status_code)  # 200 - страница загружена"
      ],
      "metadata": {
        "id": "UMiTiwiDcTs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(page.text, features=\"html.parser\")\n",
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "E4Bf__jRceK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "шаг 1 - соберем с главной страницы ссылки на те, которые хотим спарсить"
      ],
      "metadata": {
        "id": "3QwH7zace4sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = []\n",
        "\n",
        "for link in soup.find_all('a'):\n",
        "    urls.append(link.get('href'))\n",
        "\n",
        "urls\n",
        "#print(urls)\n",
        "\n",
        "# много лишних ссылок и повторов, оставим только ссылки на новости: как?"
      ],
      "metadata": {
        "id": "08GU8o_pcnG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_urls = []\n",
        "\n",
        "for link in soup.find_all('a'):\n",
        "  # здесь нужно что-то дописать...\n",
        "    full_urls.append(link.get('href'))\n",
        "\n",
        "full_urls\n",
        "# print(full_urls)"
      ],
      "metadata": {
        "id": "j5_a2Fz2c7zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# первая ссылка - попала в выдачу не совсем корректно (нам не подходит), ее можно удалить или позже указать срез ссылок для обработки, пропустив ее\n",
        "# удалим сразу\n",
        "\n",
        "del full_urls[0]\n",
        "print(full_urls)"
      ],
      "metadata": {
        "id": "kt_BuUBjg36M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "шаг 2 - посмотрим, что можно спарсить со страницы 1 любой новости"
      ],
      "metadata": {
        "id": "Zyi6cWrPe6MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url0 = full_urls[0]\n",
        "\n",
        "page0 = rq.get(url0)\n",
        "soup0 = BeautifulSoup(page0.text)\n",
        "print(soup0.prettify())"
      ],
      "metadata": {
        "id": "KroD1763c-I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# случай сложный: самая интересная информация хранится в теге meta, внутри атрибутов\n",
        "\n",
        "soup0.find_all('meta')\n",
        "#print(soup0.find_all('meta'))"
      ],
      "metadata": {
        "id": "Bxff_Dfofdde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соберем информацию об авторе, дате публикации, заголовке\n",
        "\n",
        "Какие атрибуты нам нужны?"
      ],
      "metadata": {
        "id": "Egn2nWLnfJZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# атрибуты прописаны внутри тега meta, но их 2!\n",
        "# сравните: <a href=\"ссылка\"> и <meta content=\"Михаил Подрезов\" name=\"author\"/>\n",
        "\n",
        "# обратиться к атрибутам через словари\n",
        "print(soup0.find_all('meta', {'name' : 'author'}))\n",
        "print(soup0.find_all('meta', {'name' : 'author'})[0])\n",
        "print(soup0.find_all('meta', {'name' : 'author'})[0].attrs)\n",
        "print(soup0.find_all('meta', {'name' : 'author'})[0].attrs['content'])"
      ],
      "metadata": {
        "id": "Gk9HbyJfHuh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# передадим словарь атрибутов и вызовем нужный - content\n",
        "\n",
        "author = soup0.find_all('meta', {'name' : 'author'})[0].attrs['content'] # вызовем автора по ключу (content) и сохраним в переменную\n",
        "date = soup0.find_all('meta', {'itemprop' : 'datePublished'})[0].attrs['content']\n",
        "title = soup0.find_all('meta', {'property' : 'og:title'})[0].attrs['content']\n",
        "\n",
        "print(author, date, title)"
      ],
      "metadata": {
        "id": "PnEXVZccfKYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# можно также через метод get ( = как мы извлекаем ссылку из href)\n",
        "for i in soup0.find_all('meta'):\n",
        "  print(i.get(\"content\"))\n",
        "\n",
        "  # почему этот способ не совсем подходит: нам нужно по одному атрибуту доставать содержимое (content), а по другому проверять, что это нужный нам тэг"
      ],
      "metadata": {
        "id": "oy9ESrd0IYKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in soup0.find_all('meta'):\n",
        "  if i.get('name') == 'author':\n",
        "    print(i.get(\"content\"))\n",
        "\n",
        "# в таком виде этот вариант тоже сработает:\n",
        "# найди все теги meta, и если есть атрибут name = \"author\",\n",
        "# достань содердимое атрибута content"
      ],
      "metadata": {
        "id": "_MVwJXrrziDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Извлечем текст новости, используя тэг p с атрибутом mb-6"
      ],
      "metadata": {
        "id": "hJywp95fhURo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup0.find_all('p')"
      ],
      "metadata": {
        "id": "04U7GLeA5bKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = soup0.find_all('p', {'class' : 'mb-6'})\n",
        "text_list\n",
        "#print(text_list)"
      ],
      "metadata": {
        "id": "cT6eJ8SUhRu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "for i in text_list:\n",
        "  text.append(i.text)\n",
        "text\n",
        "#print(text)\n",
        "\n",
        "# можно заменить на: text = [i.text for i in text_list]"
      ],
      "metadata": {
        "id": "oiMw6r17iTbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# слегка поправим текст:\n",
        "final_text = ' '.join(text)\n",
        "final_text = final_text.replace('\\xa0', ' ') # \\xa0 - неразрывный пробел\n",
        "final_text\n",
        "#print(final_text)"
      ],
      "metadata": {
        "id": "gg_9SEt6immG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "шаг 3: напишем функцию, которая будет идти по ссылкам из full_urls и парсить каждую страницу"
      ],
      "metadata": {
        "id": "Hi6THR89i5sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# просто напоминание, что в full_urls у нас все ссылки, а мы спарсили одну страницу\n",
        "full_urls # print(full_urls)"
      ],
      "metadata": {
        "id": "grtig6Rbhjra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GetNews(url0):\n",
        "  page0 = rq.get(url0)\n",
        "  soup0 = BeautifulSoup(page0.text, features=\"html.parser\")\n",
        "  author = soup0.find_all('meta', {'name' : 'author'})[0].attrs['content']\n",
        "  date = soup0.find_all('meta', {'itemprop' : 'datePublished'})[0].attrs['content']\n",
        "  title = soup0.find_all('meta', {'property' : 'og:title'})[0].attrs['content']\n",
        "  text_list = soup0.find_all('p', {'class' : 'mb-6'})\n",
        "  text = [i.text for i in text_list]\n",
        "  final_text = ' '.join(text)\n",
        "  final_text = final_text.replace('\\xa0', ' ')\n",
        "  return url0, author, date, title, final_text"
      ],
      "metadata": {
        "id": "IY59cx7ei4IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = [] # список с новостями\n",
        "\n",
        "for link in full_urls:\n",
        "  new = GetNews(link)\n",
        "  news.append(new)"
      ],
      "metadata": {
        "id": "IhoOb2fYjKSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(news)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "l3nBbYTWjVvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['link', 'author', 'date', 'title', 'text']"
      ],
      "metadata": {
        "id": "B-nDj4NyjY13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(1)"
      ],
      "metadata": {
        "id": "LcQpUHS2jntE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('nplus_news.xlsx')"
      ],
      "metadata": {
        "id": "6GeygSd9jxl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Как парсить сайты, не раздражая сервера"
      ],
      "metadata": {
        "id": "QQpr82aNkOOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выше мы пробовали скрейпить сайт циклом. Скорость такого скрейпинга ограничена только скоростью сети и быстродействием компьютеров. То есть может быть довольно высокой\n",
        "\n",
        "Это нагрузка на сервера, и иногда владельцы блокируют скрейперов"
      ],
      "metadata": {
        "id": "6QfkgcxbkRk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# магическая команда!\n",
        "# примеры здесь https://pythonpip.ru/osnovy/osnovnye-magicheskie-komandy-v-python-s-primerami\n",
        "\n",
        "for i in range(10): # 10 раз запроси одну и ту же страницу\n",
        "    rq.get('https://quotes.toscrape.com')"
      ],
      "metadata": {
        "id": "H8kwG5ADkgy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Использовать sleep из модуля time"
      ],
      "metadata": {
        "id": "Rwp77g5Ukcg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "St6FpgbJlAt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for i in range(10):\n",
        "    rq.get('https://quotes.toscrape.com')\n",
        "    time.sleep(1) # ждем 1 секунду после каждого шага"
      ],
      "metadata": {
        "id": "_2R3les6kPro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Мимикрировать под человека с помощью fake_useragent"
      ],
      "metadata": {
        "id": "O9xoHliKlbld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_link = 'https://spbu.ru'\n",
        "response = rq.get(page_link)\n",
        "print(response.status_code)\n",
        "\n",
        "# для нас запрещено!\n",
        "# сервер видит, что к нему пришла программа, а не человек"
      ],
      "metadata": {
        "id": "xyDGQxailfVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fake_useragent\n",
        "from fake_useragent import UserAgent\n",
        "print(UserAgent().chrome)"
      ],
      "metadata": {
        "id": "lbdRMx4KmDJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_link = 'https://spbu.ru'\n",
        "response = rq.get(page_link, headers={'User-Agent': UserAgent().chrome})"
      ],
      "metadata": {
        "id": "ufdxvq4umHUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.status_code)"
      ],
      "metadata": {
        "id": "ywW_RAhOmPSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "id": "T_BbU055mRBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Документация + пример, если надо имитировать не одного человека, а [нескольких](https://fake-useragent.readthedocs.io/en/latest/)"
      ],
      "metadata": {
        "id": "Js12xnT2kW0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. В крайних случаях можно использовать тор/прокси-серверы для смены IP\n",
        "\n",
        "* [Раз](https://habr.com/ru/company/ods/blog/346632/#22-tor---syn-odina)\n",
        "* [Два](https://stackoverflow.com/questions/30286293/make-requests-using-python-over-tor)\n"
      ],
      "metadata": {
        "id": "S19Su_LdoTOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Полезный совет про try / except!\n",
        "\n",
        "Когда парсите много страниц разом, велик риск получить ошибки (какая-то страница не ответила, сервер начал нас блокировать и т.д.)\n",
        "\n",
        "Используйте try / except"
      ],
      "metadata": {
        "id": "rWlWI_6aoGPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,10):\n",
        "    print(100/i)\n",
        "\n",
        "print('Все в порядке!')"
      ],
      "metadata": {
        "id": "BKRWxU8uoFgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,10):\n",
        "    try:\n",
        "        print(100/i)\n",
        "    except:\n",
        "        print(f'с числом {i} не работает')\n",
        "\n",
        "print('Все в порядке!')"
      ],
      "metadata": {
        "id": "Qk0Q4UPVozIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как использовать при парсинге:"
      ],
      "metadata": {
        "id": "vd-y0l-hp_EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unis_to_parse = ['hse', 'itmo', 'msu', 'hsе', 'spbu']"
      ],
      "metadata": {
        "id": "WarQ7vJ6pFBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for address in unis_to_parse:\n",
        "    page_response = rq.get(f'https://{address}.ru')\n",
        "    current_html = BeautifulSoup(page_response.text)\n",
        "    print(current_html.title)"
      ],
      "metadata": {
        "id": "JUut8t2tpGeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for address in unis_to_parse:\n",
        "    try:\n",
        "        page_response = rq.get(f'https://{address}.ru')\n",
        "        current_html = BeautifulSoup(page_response.text)\n",
        "        print(current_html.title)\n",
        "    except:\n",
        "        print(f'С сайтом https://{address}.ru не работает')\n",
        "\n",
        "# hse - во втором случае использована е кириллицей\n",
        "# spbu - парсинг запрещен, но с fake_useragent получится"
      ],
      "metadata": {
        "id": "gUowN922pI1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF72_zk5ak4g"
      },
      "source": [
        "# Полезные ссылки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtTvDwFoqGox"
      },
      "source": [
        "Что почитать об использовании данных\n",
        "- [как устроен сбор данных](https://en.wikipedia.org/wiki/Web_scraping)\n",
        "\n",
        "\n",
        "- закон об авторском праве ([в деталях](http://www.consultant.ru/document/cons_doc_LAW_64629/0b318126c43879a845405f1fb1f4342f473a1eda/), [вкратце](https://ru.wikipedia.org/wiki/%D0%90%D0%B2%D1%82%D0%BE%D1%80%D1%81%D0%BA%D0%BE%D0%B5_%D0%BF%D1%80%D0%B0%D0%B2%D0%BE_%D0%B2_%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D0%B8))\n",
        "- [закон о персональных данных](http://www.consultant.ru/document/cons_doc_LAW_61801/)\n",
        "- [типы лицензирования данных](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository)\n",
        "- [FAIR data](https://en.wikipedia.org/wiki/FAIR_data)\n",
        "- [OpenData](https://en.wikipedia.org/wiki/Open_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlEvq5ofqGox"
      },
      "source": [
        "Гайды и туториалы\n",
        "\n",
        "- [документация requests и быстрый гайд](https://requests.readthedocs.io/en/master/user/quickstart/)\n",
        "\n",
        "\n",
        "- [документация Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "\n",
        "- [text-only](https://sjmulder.nl/en/textonly.html) страница содержит веб-сайты, чтобы легко начать парсить, но страница не живая + ее [кешированная версия](https://web.archive.org/web/20220827013141/https://sjmulder.nl/en/textonly.html)\n",
        "\n",
        "\n",
        "- [здесь](https://www.york.ac.uk/teaching/cws/wws/webpage1.html) можно почитать про структуру html подробнее\n",
        "\n",
        "\n",
        "- [здесь](https://www.w3schools.com/html/html_examples.asp) еще и потренироваться в режиме онлайн (с этой ссылки мы начали занятие)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILp0Ke2daokq"
      },
      "source": [
        "Чем парсить соцсети (не исчерпывающий список)\n",
        "\n",
        "(стало сложнее, в России 1 и 2 ссылка не откроются без VPN...)\n",
        "\n",
        "- [Twitter](https://developer.twitter.com/en/docs/twitter-api/tools-and-libraries/v2)\n",
        "- [Meta](https://developers.facebook.com/docs/graph-api/)\n",
        "- [VK](https://vk-api.readthedocs.io/en/latest/), [положения о прайваси](https://vk.com/dev/uprivacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задания\n",
        "\n",
        "1. Спарсите ВСЕ таблички со страницы https://en.wikipedia.org/wiki/List_of_nuclear_weapons_tests\n",
        "\n",
        "Каждую табличку можно передать в датафрейм и сохранить в отдельный файл"
      ],
      "metadata": {
        "id": "TuKaEA0iF4Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wZj4a6g0F9UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5k9p2myGJGTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SIN_Z6ITJGLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Парсинг цитат с сайта:\n",
        "\n",
        "https://quotes.toscrape.com\n",
        "\n",
        "Спарсите все 10 страниц цитат и сохраните все цитаты в датафрейм / текстовый файл на ваш выбор"
      ],
      "metadata": {
        "id": "b_OBjvFPjwpl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "66l0Gyu7pRf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_cQKAM-1pQxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KBGvK9GOpQqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Раньше была еще одна прекрасная подборка сайтов для скрейпинга, но она ушла в веб-архив:\n",
        "\n",
        "https://web.archive.org/web/20220827013141/https://sjmulder.nl/en/textonly.html\n",
        "\n",
        "Сохраните все ссылки на эти сайты для тернировок по скрейпингу, убрав из их адресов доменное имя web.archive"
      ],
      "metadata": {
        "id": "oXwud6BvGHIK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSND7QwOGZ-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-ULKIq8JG63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJEKDdo5JG18"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
