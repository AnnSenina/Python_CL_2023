{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnSenina/Python_CL_2023/blob/main/notebooks/%D0%9F%D0%B5%D1%80%D1%81%D0%BE%D0%BD%D0%B0%D0%B6%D0%B8%20NER_%D1%87%D0%B0%D1%81%D1%82%D1%8C_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1xyT2Iv6a68"
      },
      "source": [
        "# Просто запускаем все кнопки подряд"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkmvMdQ36iXb"
      },
      "outputs": [],
      "source": [
        "# для очистки (препроцессинга) текста...\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "ru_stop_words = stopwords.words('russian') # давайте для русского сохранять в отдельную переменную\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# pymorphy\n",
        "!pip install pymorphy2\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "morph = MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzTrKcu2UW2U"
      },
      "outputs": [],
      "source": [
        "!pip install natasha\n",
        "\n",
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "\n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "\n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "\n",
        "    Doc\n",
        ")\n",
        "\n",
        "\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "names_extractor = NamesExtractor(morph_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy7-8YU55el5"
      },
      "outputs": [],
      "source": [
        "!pip install -U pip setuptools wheel --user\n",
        "!pip install -U spacy\n",
        "!python -m spacy download ru_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Внимание! Запустите ТОЛЬКО русский или английский, чтобы они не съели друг друга"
      ],
      "metadata": {
        "id": "QJYuei3LUIdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkldWZF07KFZ"
      },
      "outputs": [],
      "source": [
        "# для русского языка\n",
        "import spacy\n",
        "nlp_ru = spacy.load('ru_core_news_sm')\n",
        "\n",
        "def get_ru_lemmas(text):\n",
        "  text = text.lower()\n",
        "  text_tokens = word_tokenize(text)\n",
        "  clean_tokens = []\n",
        "  for i in text_tokens:\n",
        "    if i[0].isalpha() and i not in ru_stop_words:\n",
        "      clean_tokens.append(i)\n",
        "  ru_lemmatized = []\n",
        "  for word in clean_tokens:\n",
        "    result = morph.parse(word)\n",
        "    most_probable_result = result[0]\n",
        "    normal_form = most_probable_result.normal_form\n",
        "    ru_lemmatized.append(normal_form)\n",
        "  return ru_lemmatized"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# для английского запустите эту ячейку\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_eng_lemmas(text):\n",
        "  text = text.lower()\n",
        "  text_tokens = word_tokenize(text)\n",
        "  clean_tokens = [] # чистим список токенов\n",
        "  for i in text_tokens:\n",
        "    if i[0].isalpha() and i not in stop_words:\n",
        "      clean_tokens.append(i)\n",
        "  spacy_token = []\n",
        "  doc = nlp(' '.join(clean_tokens))\n",
        "  for token in doc:\n",
        "    if token.lemma_ != '-' and token.lemma_ != '.':\n",
        "        spacy_token.append(token.lemma_)\n",
        "  return spacy_token"
      ],
      "metadata": {
        "id": "fL-8faT-xx4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ2dkAPvMqmJ"
      },
      "source": [
        "# Важно! Ниже нужно изменить название текста на ваш"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P_2qt9HUk03"
      },
      "outputs": [],
      "source": [
        "with open('Dostoevsky_PrestuplenieINakazanie.txt', 'r', encoding='utf-8') as f: # впишите ваше название.txt вместо Достоевского\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0JSBAAa5Yzm"
      },
      "source": [
        "# NER natasha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ8wkioE6fSB"
      },
      "outputs": [],
      "source": [
        "doc = Doc(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdM75aCSVSJi"
      },
      "outputs": [],
      "source": [
        "doc.segment(segmenter)\n",
        "print(doc.tokens[:5])\n",
        "print(doc.sents[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLvIAnj3U7d9"
      },
      "outputs": [],
      "source": [
        "doc.tag_ner(ner_tagger)\n",
        "print(doc.spans[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94sSgjrpVBGy"
      },
      "outputs": [],
      "source": [
        "for span in doc.spans:\n",
        "  span.normalize(morph_vocab)\n",
        "  print(span)\n",
        "  print(span.text)\n",
        "  print(span.type)\n",
        "{_.text: _.normal for _ in doc.spans if _.text != _.normal}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qjh40z2D5Bvf"
      },
      "outputs": [],
      "source": [
        "# Важно!!!\n",
        "# вы можете поменять 'PER' на 'ORG' или 'LOC', чтобы собрать огранизации и локации\n",
        "\n",
        "persons = []\n",
        "for span in doc.spans:\n",
        "  if span.type == 'PER' and span.text not in persons: # для простоты в этой строке можно поменять только одно слово\n",
        "\n",
        "  # еще! еще вам нужно извлечь, например, персон ИЛИ локации (все вместе), можно еще так:\n",
        "  # if (span.type == 'PER' or span.type == 'LOC') and span.text not in persons:\n",
        "\n",
        "    persons.append(span.text)\n",
        "persons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tZ_Jz565X_I"
      },
      "source": [
        "# NER spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUdy7KvR5tVz"
      },
      "outputs": [],
      "source": [
        "doc = nlp_ru(text[:1000000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCQqrMXb51Mv"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "displacy.render(list(doc.sents)[1], style='dep')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKgOTiiv53ho"
      },
      "outputs": [],
      "source": [
        "for ent in doc.ents:\n",
        "  print(ent.lemma_, '->', ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa50v-9R6Bcy"
      },
      "outputs": [],
      "source": [
        "LOC = []\n",
        "PER = []\n",
        "ORG = []\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ == 'LOC':\n",
        "        if ent.lemma_ not in LOC:\n",
        "            LOC.append(ent.lemma_)\n",
        "    if ent.label_ == 'PER':\n",
        "         if ent.lemma_ not in PER:\n",
        "            PER.append(ent.lemma_)\n",
        "    if ent.label_ == 'ORG':\n",
        "        if ent.lemma_ not in ORG:\n",
        "            ORG.append(ent.lemma_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPBMvzdOHHsb"
      },
      "outputs": [],
      "source": [
        "PER # на самом деле, spaCy отдает вам и персон (PER), и организации (ORG), и локации (LOC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by6eZMJR9ND2"
      },
      "source": [
        "# Сравниваем natasha и spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xpot3Loz9SMs"
      },
      "outputs": [],
      "source": [
        "# я бы уже полученные NER от natasha лемматизировала... - это уберет лишние разночтения и глюки\n",
        "person_natasha = []\n",
        "for i in persons:\n",
        "  person_natasha.append(' '.join(get_ru_lemmas(i)))\n",
        "person_natasha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kD4BlFi-iVp"
      },
      "outputs": [],
      "source": [
        "# Важно!!!\n",
        "# если у вас что-то сломалось, нужно посмотреть, что именно:\n",
        "# person_natasha отсутствуют -> запускаем: res_person = list(set(PER)) вместо кода ниже\n",
        "# list(set(PER)) отсутсвуют -> запускаем: res_person = set(person_natasha) вместо кода ниже\n",
        "# а если вам по-прежнему нужны персоны ИЛИ локации (все вместе): res_person = list(set(PER) | set(LOC))\n",
        "# + можно пересечь персон + локации из Наташи и спейси: res_person = list((set(PER) | set(LOC) & set(person_natasha))\n",
        "\n",
        "# Код ниже ТОЛЬКО для тех, у кого оба списка нашли много персон\n",
        "\n",
        "res_person = sorted(list(set(PER) & set(person_natasha))) # можно еще наташу не трогать, но PER поменять на ваше LOC или ORG\n",
        "res_person"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtRQdOmofxr6"
      },
      "source": [
        "Что с ними делать дальше?\n",
        "\n",
        "Можно автоматически собрать словарь, но все равно лишнее осталось"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTrTq761f5sG"
      },
      "outputs": [],
      "source": [
        "dict_person = {}\n",
        "for i in res_person:\n",
        "  if i not in dict_person and ' ' not in i and '-то' not in i:\n",
        "    dict_person[i] = [i]\n",
        "  elif i not in dict_person and i[:i.find('-то')] not in dict_person and '-то' in i and i.find('-то') < i.find(' '):\n",
        "    dict_person[i[:i.find('-то')]] = [i]\n",
        "  elif i not in dict_person and i[:i.find('-то')] in dict_person and '-то' in i:\n",
        "    dict_person[i[:i.find('-то')]] = dict_person[i[:i.find('-то')]] + [i]\n",
        "  elif i not in dict_person and i[:i.find(' ')] not in dict_person and ' ' in i:\n",
        "    dict_person[i[:i.find(' ')]] = [i]\n",
        "  elif i not in dict_person and i[:i.find(' ')] in dict_person:\n",
        "    dict_person[i[:i.find(' ')]] = dict_person[i[:i.find(' ')]] + [i]\n",
        "  elif i not in dict_person and i[:i.rfind(' ')] in dict_person:\n",
        "    dict_person[i[:i.find(' ')]] = dict_person[i[:i.rfind(' ')]] + [i]\n",
        "dict_person"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRU0dQgae751"
      },
      "outputs": [],
      "source": [
        "for i in dict_person:\n",
        "  if i not in dict_person[i]:\n",
        "    dict_person[i] += [i]\n",
        "\n",
        "dict_person"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4SZ0wzdSCr2"
      },
      "source": [
        "# Здесь придется чистить руками\n",
        "\n",
        "сохраняем, ищем файл network.txt - чистим руками"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWApPWDxSGOu"
      },
      "outputs": [],
      "source": [
        "with open('network.txt', 'w', encoding='utf-8') as f:\n",
        "  for i in dict_person:\n",
        "    print(i, *dict_person[i], sep=';', file=f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Как чистить:\n",
        "\n",
        "персона;персона;другое имя1;другое имя2....\n",
        "\n",
        "станет:\n",
        "\n",
        "\n",
        "персона : как мы ее назваем = персона, другое имя1, другое имя2..."
      ],
      "metadata": {
        "id": "CEEjkuyCVSNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Еще сразу пример про сеть терминов:\n",
        "\n",
        "комсомол;разные варианты\n",
        "\n",
        "Например:\n",
        "\n",
        "комсомол;комсомол;союз молодежи;приводной ремень...\n",
        "\n",
        "В этом случае комсомол;комсомол - это не ошибка: под словом комсомол мы имеем в виду имя точечки в сети, а также один из возможных вариантов, как его могут упомянуть в тексте"
      ],
      "metadata": {
        "id": "Hi2GZN6yCxk1"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}