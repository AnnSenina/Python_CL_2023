{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnSenina/Python_CL_2023/blob/main/notebooks/Python_9_%D0%BF%D1%80%D0%B5%D0%BF%D1%80%D0%BE%D1%86%D0%B5%D1%81%D1%81%D0%B8%D0%BD%D0%B3%2C_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Препроцессинг NLTK\n",
        "\n",
        "Подготовка текста для анализа\n",
        "\n",
        "<s>Ведь мы c вами все знаем, что на самом деле цифровой анализ текста - это и есть частотности слов</s> :)\n",
        "\n",
        "Используются материалы из тетрадок Д.Скоринкина, А. Хорошевой"
      ],
      "metadata": {
        "id": "xKMFDWff6OJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQfRhCxL3yA1"
      },
      "outputs": [],
      "source": [
        "# кое-что мы уже умеем, например:\n",
        "\n",
        "print(\" Давайте нормализуем этот текст!      \".lower().strip(\" )?!.\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# функция, убирающая пунктуацию\n",
        "\n",
        "import string\n",
        "\n",
        "def normalize(text):\n",
        "    normalized = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
        "    return normalized\n",
        "\n",
        "text = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.\"\n",
        "\n",
        "print(normalize(text))\n",
        "print(normalize('Привет, здравствуй мир!'))"
      ],
      "metadata": {
        "id": "HL8oD4Rm6uR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод .maketrans() имеет три аргумента и создает таблицу перевода:\n",
        "\n",
        "* какие символы переводить (первый аргумент)\n",
        "* в какие переводить (второй аргумент)\n",
        "* какие символы удалять (третий аргумент)\n",
        "Метод .translate() использует таблицу перевода, чтобы превратить символы в новые символы.\n",
        "\n",
        "В нашем случае, в методе .maketrans() первые два аргумента мы оставляем пустыми (ничто переводится в ничто), а аргумент для удаления определяем как строку punctuation, в которой содержатся все символы пунктуации."
      ],
      "metadata": {
        "id": "pZ3Pm9ey8Qgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Задача 1: вспомним то, что уже умеем"
      ],
      "metadata": {
        "id": "vPvYUki89ZHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем лучшего писателя отзывов.\n",
        "У вас есть четрые отзыва на фильм С.Кубрика \"Сияние\". Вам нужно определить, кто из авторов отзывов написал отзыв с наибольшим количеством уникальных слов.\n",
        "    \n",
        "    Для определения вам нужно: (для каждого автора)\n",
        "\n",
        "    1. Предобработать строку, сведя все к нижнему регистру, убрать пунктуацию.\n",
        "    2. Превратить строку в список\n",
        "    3. Оставить уникальные элементы в списке (превратить список во множество)\n",
        "    4. Определить размер такого множества\n",
        "    \n",
        "У кого из авторов количество уникальных слов наибольшее?\n"
      ],
      "metadata": {
        "id": "1Yf1XqpY-Cdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paul = \"\"\"When this film first came out in 1980, I remember going to see it on opening night. This movie just scared the life out of me, which is what still happens every time\n",
        "I rent the video for a re-watch.I have seen The Shining at least six or seven times, and I still believe it to be simultaneously and paradoxically one of the most frightening and yet funniest films I've ever seen. Frightening because of the extraordinarily effective use of long shots to create feelings of isolation, convex lens shots to enhance surrealism, and meticulously scored music to bring tension levels to virtually unbearable levels. And \\\"funny\\\" because of Jack Nicholson's outrageous and in many cases ad-libbed onscreen antics. It never ceases to amaze me how The Shining is actually two films in one, both a comedy AND a horror flick. Ghostly apparitions of a strikingly menacing nature haunt much of the first half of the film, which gradually evolve into ever more serious physical threats as time progresses. Be that as it may, there is surprisingly little violence given the apparent intensity, but that is little comfort\n",
        "for the feint of heart as much of the terror is more implied than manifest. The Shining is a truly frightening movie that works symbolically on many levels, but is basically  about human shortcomings and the way they can be exploited by unconscious forces combined with weakness of will. This film scares the most just by using suggestion to turn your own imagination against you. The Shining is a brilliant cinematic masterpiece, the likes of which have never been seen before or since. Highly, highly recommended.\"\"\"\n",
        "\n",
        "jane = \"\"\"Chilling, majestic piece of cinematic fright, this film combines all the great elements of an intellectual thriller, with the grand vision of a director who has the instinctual capacity to pace a moody horror flick within the realm of his filmmaking genius that includes an eye for the original shot, an ice-cold soundtrack and an overall sense of dehumanization. This movie cuts through all the typical horror\n",
        "movies like a red-poker through a human eye, as it allows the viewer to not only feel the violence and psychosis of its protagonist, but appreciate the seed from which the derangement stems. One of the scariest things for people\n",
        "to face is the unknown and this film presents its plotting with just that thought in mind. The setting is perfect, in a desolate winter hideaway. The quietness of the moment is a character in itself, as the fermenting aggressor in Jack Torrance's mind wallows in this idle time, and breeds the devil's new playground. I always felt like the presence of evil was dormant in all of our minds, with only the circumstances of the moment, and the reasons given therein, needed to wake its violent ass and pounce over its unsuspecting victims. This film is a perfect example of this very thought.\"\"\"\n",
        "\n",
        "kate = \"\"\"What can I say about the scariest movie I have ever seen that has not already been said by others more articulate than yours truly? Do not view this film expecting to see a screen version of the Stephen King novel.\n",
        "Rather, this is a Stanley Kubrick film, and to fully appreciate it one should judge it within the context of Kubrick's entire body of work as a serious filmmaker. Thematically, THE SHINING relates most closely to 2001:\n",
        "A SPACE ODYSSEY, though flourishes of PATHS OF GLORY, A CLOCKWORK ORANGE and BARRY LYNDON do manage to figure prominently in the film's overall technique. In a nutshell (no pun intended), Jack Nicholson and Shelly Duvall co-star with Oregon's Timberline Lodge - enlisted to portray the exterior of the Overlook Hotel - in a story that appears on the surface to be about ghosts and insanity, but deals with issues of child abuse,\n",
        "immortality and duality. What the film might lack initially in terms of coherence is more than made up for in technique. Garrett Brown (the male voice in those old Molson Golden commercials), inventor of the Steadicam,chases young Danny Lloyd through hotel corridors and an amazing snow maze, providing magic-carpet-ride fluidity to scenes that ten years earlier would have been impossible to accomplish. If the film starts off too slow, remember who the director is. This man likes to take his time, and the results are well worth it: incredible aerial shots of the Overlook Hotel; horrific Diane Arbus-inspired twins staring directly at us; portentous room 237 and its treasure trove of terrible secrets; elevators that gush rivers of blood in slow-motion; Jack Torrance's immortality found via the hotel (akin to David Bowman's journey through the Space Gate); and some of the best use of pre-existing music ever assembled for a motion picture.\"\"\"\n",
        "\n",
        "nick = \"\"\"I was never a big fan of horror movies. They usually try cheap tricks to scare their audiences like loud noises and creepy children. They usually lack originality and contain overacting galore. The only horror movie i\n",
        "like was Stir of Echoes with Kevin Bacon. It was well-acted, and had a great story. But it has been joined and maybe even surpassed by Stanley Kubrick's The Shining, quite possibly the scariest movie ever. The movie follows a writer (Jack Nicholson) and his family who agree to watch over a hotel while it is closed for the winter. There were rumors of the place being haunted and the last resident went crazy and murdered his family. But Jack is convinced it will be OK and he can use the quiet to overcome his writer's block. After months of solitude and silence however, Jack becomes a grumpy and later violent. Is it cabin fever or is there something in the hotel that is driving him mad? One of the creepiest parts about the movie is the feeling of isolation that Kubrick makes. The hotel is very silent, and the rooms are huge, yet always empty. It is also eerily calm when Jack's son is riding his bike through the barren hallways. Jack Nicholson's performance is also one of his very best, scaring the hell out of me and making me sure to get out once in awhile. My favorite scene is when he is talking to a ghost from inside a walk-in refrigerator. The Shining is tops for horror movies in my opinion, beating the snot out of crap like the Ring and The Blair Witch Project. It may be a oldie, but is definitely a goodie.\"\"\""
      ],
      "metadata": {
        "id": "sM8a9Tlw-AEu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# мы можем использовать тройные кавычки (то есть три одинарные кавычки или три двойные кавычки) для строк с одинарными и двойными кавычками\n",
        "# так мы исключаем необходимость экранирования любых кавычек\n",
        "# а еще можем записывать перенос строки без экранирования \\n"
      ],
      "metadata": {
        "id": "12FjlgZz--bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ваше решение ниже:\n",
        "\n"
      ],
      "metadata": {
        "id": "PVPLIyML_3fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Давайте попробуем подготовить текст романа \"Преступление и наказание\" к анализу\n",
        "\n",
        "Проведем предобработку текста. Посмотрим на практике, на каком этапе нужна лемматизация."
      ],
      "metadata": {
        "id": "75EnFxyh9CNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Dostoevsky_PrestuplenieINakazanie.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "Rt48CpY_7G3R"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()"
      ],
      "metadata": {
        "id": "tzuGUmxNB9w2"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Убедимся, что в тексте лежит то что мы ожидаем:"
      ],
      "metadata": {
        "id": "bViid1pcCC2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# начало романа:\n",
        "print(text[:100])"
      ],
      "metadata": {
        "id": "1HDYpUUICBDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получим слова\n",
        "\n",
        "* Их частотности\n",
        "* Их сочетаемость друг с другом\n",
        "* Их распределение по тексту и т.д."
      ],
      "metadata": {
        "id": "VSSKHEv_Cu87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы знаем простой способ токенизировать строку — метод .split:"
      ],
      "metadata": {
        "id": "E3zx-YjOEpGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь разделим \"Преступление и наказание\" и посчитаем в нем слова — хотя бы примерно:"
      ],
      "metadata": {
        "id": "Gts2Q-QcEyk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = text.split()\n",
        "print('Примерное количество слов в \"Преступлении и наказании\":', len(text))\n",
        "# запомним это число..."
      ],
      "metadata": {
        "id": "F_do8eUlEula"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# посмотрим на верхушку этого списка\n",
        "print(text_list[:30])"
      ],
      "metadata": {
        "id": "5TRW36KRFBjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# посмотрим на все слова, в которых есть подстрока 'топор'\n",
        "for token in text_list:\n",
        "  if 'топор' in token:\n",
        "    print(token)"
      ],
      "metadata": {
        "id": "jSV0ykmGFkud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Более умный способ токенизации: делим текст регулярным выражением\n"
      ],
      "metadata": {
        "id": "JSihridMFwQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text_list_with_re = re.split(r\"[-@\\s.,)(\\\":;!?–\\n]+\", text)"
      ],
      "metadata": {
        "id": "ygQFU5L3F0N-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_with_re[:30]"
      ],
      "metadata": {
        "id": "W5KBGxzkF_RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# проверим, что проблема приклеившейся пунктуации ушла\n",
        "for word in text_list_with_re:\n",
        "  if 'топор' in word:\n",
        "    print(word)"
      ],
      "metadata": {
        "id": "SczKlOqaGEa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = re.sub(r'\\W', ' ', text).split()\n",
        "print(len(res)) # в прошлый раз получилось 1094386\n",
        "# запомним это число тоже"
      ],
      "metadata": {
        "id": "VwKUIXyyBNXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Еще более умный способ: сегментируем текст готовым токенизатором — возьмем его из прекрасной библиотеки для обработки языка NLTK\n",
        "\n",
        "[Документация по NLTK](https://www.nltk.org/) и [книжка](https://www.nltk.org/book/)"
      ],
      "metadata": {
        "id": "HH5xf4oiGOHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "# в Colab уже есть, в других средах - установите\n",
        "# напоминаю, что в PyCharm мы устанавливаем в терминале без !"
      ],
      "metadata": {
        "id": "SCwW6hEo-8f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# все библиотеки и импорты на сегодня\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "\n",
        "# это подгрузка вспомогательных данных, которые нужны nltk для токенизации\n",
        "from nltk import download\n",
        "download('punkt')\n",
        "\n",
        "# стоп-слова\n",
        "download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('russian')\n",
        "\n",
        "# стемминг\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "# разделение на предложения\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "gE1JTGwiGo9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = \"\"\"Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE. Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\n",
        "Во-первых, NLI можно использовать для контроля качества генеративных моделей. Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод. Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\". С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\n",
        "Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов. Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше. В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично. Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\"\"\"\n",
        "# текст отсюда - https://habr.com/ru/post/582620/"
      ],
      "metadata": {
        "id": "Zjy_KO28T1Qz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordpunct_tokenize(text_2))\n",
        "# wordpunct_tokenizer разбирает по регулярке - '\\w+|[^\\w\\s]+'\n",
        "# word_tokenize - тоже основан на регулярках, но более умных (учитывается последовательность некоторых символов, символы начала, конца слова и т.д).\n",
        "# для русского языка работает немного хуже, чем для английского"
      ],
      "metadata": {
        "id": "NPgNbDEgT177"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text_2))"
      ],
      "metadata": {
        "id": "i6dPXuIM0T4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# давайте быстро найдем различия:\n",
        "set(wordpunct_tokenize(text_2)) ^ set(word_tokenize(text_2))"
      ],
      "metadata": {
        "id": "nVfJ26Fh1QZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(wordpunct_tokenize(text_2)))"
      ],
      "metadata": {
        "id": "7_TjRBWv0qRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(word_tokenize(text_2)))"
      ],
      "metadata": {
        "id": "L13DAZ3P0rc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# вернемся к Достоевскому, используем более умную лемматизацию\n",
        "text_list_nltk = word_tokenize(text)\n",
        "print(text_list_nltk[:30])"
      ],
      "metadata": {
        "id": "7M8E1sxJGwnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Посчитаем частотность слов в нашем списке слов"
      ],
      "metadata": {
        "id": "H4nIAbE9HRao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы могли бы написать цикл, который считает упоминания каждого слова, перебирая в списке токенов слово за словом. А затем посчитать их при помощи словаря.\n",
        "\n",
        "Но! Мы воспользуемся встроенным в Python объектом Counter; он все это сделает сам."
      ],
      "metadata": {
        "id": "9vMjWIrgHcFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counter -- встроенный счетчик частотностей в Python"
      ],
      "metadata": {
        "id": "ZPmJIpx9HnhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "pY3X7cnOHXq9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counter - это специальный объект, который умеет легко считать повторяющиеся элементы в iterable\n",
        "# Например в строке:\n",
        "char_freqs = Counter('aaaaabbbbcccddefghik') # получим частотности всех элементов строки\n",
        "print(char_freqs)\n",
        "print(char_freqs.most_common(3))"
      ],
      "metadata": {
        "id": "4pYvF8oMHqx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freqs = Counter(text_list_nltk) # отправим в Counter\n",
        "print(word_freqs.most_common(10)) # получим топ 10 слов"
      ],
      "metadata": {
        "id": "sUnjcX-cIAX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Снова удалим пунктуацию"
      ],
      "metadata": {
        "id": "u7oNBTGlINjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# вспомним старые способы\n",
        "\n",
        "print('john'.isalpha())\n",
        "print('1989'.isalpha())\n",
        "print(','.isalpha())\n",
        "print('диван-кровать'.isalpha()) # из-за дефиса тоже будет false"
      ],
      "metadata": {
        "id": "lvELGmdhIfFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# методом maketrans, посмотрим на string.punctuation\n",
        "print(string.punctuation) # не хватает длинного тире"
      ],
      "metadata": {
        "id": "c73a3bPASykt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_without_punkt = []\n",
        "for word in text_list_nltk:\n",
        "    if word[0].isalpha():\n",
        "        text_without_punkt.append(word)\n",
        "\n",
        "# на практике вы часто встретите строковые включения:\n",
        "# text_without_punkt = [word for word in text_list_nltk if word[0].isalpha()]\n",
        "# краткая форма записи цикла for"
      ],
      "metadata": {
        "id": "ZkmeiGNAI1kL"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_without_punkt[:30])"
      ],
      "metadata": {
        "id": "yQuKu10OJGz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text_without_punkt))\n",
        "# 1094386 получалось обычным .split()\n",
        "# 173393 получилось с использованием простой регулярки"
      ],
      "metadata": {
        "id": "Gx0XzdxzAp-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Посмотрим на частотность слов без пунктуации"
      ],
      "metadata": {
        "id": "FrbsmxrIJNsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(text_without_punkt).most_common(30))"
      ],
      "metadata": {
        "id": "0xhQQ7QwJRxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Чистим от стоп-слов"
      ],
      "metadata": {
        "id": "O0jIlYlbJhMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно загрузить свои стоп-слова и удалить их, но проще взять из NLTK: там есть наборы стоп-слов для разных языков\n"
      ],
      "metadata": {
        "id": "D_Anp2_nJkVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "print(stopwords.fileids())"
      ],
      "metadata": {
        "id": "5GGXQfOOJtt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('russian')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "48lj7mrL6Zoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_clean = []\n",
        "for word in text_without_punkt:\n",
        "  if word not in stop_words:\n",
        "    text_clean.append(word)\n",
        "\n",
        "# text_clean = [word for word in text_without_punkt if word not in stop_words]"
      ],
      "metadata": {
        "id": "M9EtTRs8J5Jl"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_clean[:200])"
      ],
      "metadata": {
        "id": "eo7Ee7SEKSj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text_clean))"
      ],
      "metadata": {
        "id": "D7XX1ICa-v2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(text_clean).most_common(50))"
      ],
      "metadata": {
        "id": "ngpNxGtqKo_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Стемминг в Python\n",
        "\n",
        "Самый простой способ автоматический нормализации слов в языках с морфологией — стемминг. Стемминг — это очень грубое разбиение формы на предполагаемую основу и предполагаемую флексию.\n",
        "\n",
        "Программы-стеммеры умеют превращать:\n",
        "* \"Vyshka's students coded\" в \"Vyshka student code\"\n",
        "* 'Маша поехала за грибами' в 'Маш поехал за гриб'\n",
        "* 'Даня работает в Вышке' в \"Дан работа в Вышк\"\n",
        "\n",
        "Как можно догадаться из этих примеров, стемминг — не лучшее (и крайне непопулярное) решение для языков типа русского. Он лучше подходит для английского.\n",
        "\n",
        "В NLTK есть готовая реализация стеммера для русского языка. Давайте потестируем ее"
      ],
      "metadata": {
        "id": "Jzd9pOcYK4QJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK стемминг"
      ],
      "metadata": {
        "id": "KD6lyVLfLJDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Самый известный стеммер - стеммер Портера (или snowball стеммер).\n",
        "\n",
        "Подробнее про стеммер Портера можно почитать [вот тут](https://medium.com/@eigenein/стеммер-портера-для-русского-языка-d41c38b2d340)\n",
        "\n",
        "А совсем подробнее [вот тут](http://snowball.tartarus.org/algorithms/russian/stemmer.html)"
      ],
      "metadata": {
        "id": "eC3HE7FsVOca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"russian\") # в эту переменную мы сохраним уже готовый объект-стеммер для русского"
      ],
      "metadata": {
        "id": "OrZ65rHiLGu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## предположите, что выдаст?\n",
        "print(stemmer.stem('университетами'))\n",
        "print(stemmer.stem('мышам'))\n",
        "print(stemmer.stem('конями'))\n",
        "print(stemmer.stem('людей'))"
      ],
      "metadata": {
        "id": "deFj6YY_LRZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## стеммер не умеет сам токенизировать -- он работает только с отдельными словами.\n",
        "## поэтому надо идти по словам циклом\n",
        "text_stemmed = []\n",
        "for word in text_clean:\n",
        "  text_stemmed.append(stemmer.stem(word))\n",
        "\n",
        "# аналогично text_stemmed = [stemmer.stem(word) for word in text_clean]\n",
        "print(text_stemmed[:20])"
      ],
      "metadata": {
        "id": "k8qc06_DLTIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(text_stemmed).most_common(50))"
      ],
      "metadata": {
        "id": "xr30gE4_P23o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сегментация на предложения\n",
        "\n",
        "NLTK также умеет разбивать текст на предложения"
      ],
      "metadata": {
        "id": "yVP3LUxI-sXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.tokenize import sent_tokenize\n",
        "\n",
        "print(sent_tokenize(text)[:20])"
      ],
      "metadata": {
        "id": "ROOz9MV9_sqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Доп. задания\n",
        "\n"
      ],
      "metadata": {
        "id": "V8HSb8QdPwZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. соберем функцию, которая берет текст (строку), приводит к нижнему регистру, токенизирует, удаляет пунктуацию и стоп-слова"
      ],
      "metadata": {
        "id": "k5R3RVDg8BSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tokens(text):\n",
        "  # ваш код ниже\n",
        "\n",
        "\n",
        "  return text_clean"
      ],
      "metadata": {
        "id": "cZgyi7YT8As2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# не забудьте испытать его на любом тексте, можно на Достоевском\n",
        "\n",
        "print(clean_tokens(text))"
      ],
      "metadata": {
        "id": "yHlhch888eHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. давайте поизвлекаем текст из датасетов и поисследуем его, например, составив частотный список (с нормализацией, удалением пунктуации, токенизацией и удалением стоп-слов)\n",
        "\n",
        "Датасет [здесь](https://github.com/AnnSenina/Python_CL_2023/blob/main/data/elonmusk.csv)"
      ],
      "metadata": {
        "id": "-hx0FkoNP_MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('elonmusk.csv', encoding=\"utf-8\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "mqIfAEZNQKfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"tweet\"] # в этой колонке нужные нам данные"
      ],
      "metadata": {
        "id": "Getf_gdxRZLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# как достать текст из колонок датафрейма\n",
        "tweets = df[\"tweet\"].to_list()\n",
        "print(tweets)"
      ],
      "metadata": {
        "id": "FVG3ZZwwRc1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# дальше работаем с переменной tweets\n",
        "# ваш код ниже\n",
        "\n"
      ],
      "metadata": {
        "id": "rsq1lJH5RjxM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
